{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipped loading some Tensorflow models, missing a dependency. No module named 'tensorflow'\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. No module named 'torch_geometric'\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. cannot import name 'DMPNN' from 'deepchem.models.torch_models' (C:\\Users\\dust\\AppData\\Roaming\\Python\\Python310\\site-packages\\deepchem\\models\\torch_models\\__init__.py)\n",
      "Skipped loading some Jax models, missing a dependency. No module named 'jax'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import deepchem as dc\n",
    "import torch\n",
    "import lightning as L\n",
    "\n",
    "from rdkit import Chem\n",
    "from torch.utils.data import DataLoader\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "\n",
    "from model.demolta import DeMOLTaCollateFn, DeMOLTaConfig\n",
    "from trainer import LitDeMOLTaForSMILESClassification, LitDeMOLTaGeneratorForSMILESClassification, LitDeMOLTaDiscriminatorForSMILESClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./dataset/data/train.csv')\n",
    "test_df = pd.read_csv('./dataset/data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_smiles = train_df['SMILES'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs = np.zeros(len(train_smiles))\n",
    "Ys = np.ones(len(train_smiles))\n",
    "dataset = dc.data.DiskDataset.from_numpy(X=Xs,y=Ys,w=np.zeros(len(train_smiles)),ids=train_smiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaffoldsplitter = dc.splits.ScaffoldSplitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices, val_indices, _ = scaffoldsplitter.split(\n",
    "    dataset=dataset,\n",
    "    frac_train=0.8,\n",
    "    frac_valid=0.2,\n",
    "    frac_test=0.0,\n",
    "    seed=SEED,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = []\n",
    "for train_index in train_indices:\n",
    "    smiles = train_df['SMILES'].iloc[train_index]\n",
    "    labels = train_df['MLM'].iloc[train_index]\n",
    "    sample = {\n",
    "        'mol': Chem.MolFromSmiles(smiles),\n",
    "        'labels' : labels\n",
    "    }\n",
    "    train_dataset.append(sample)\n",
    "\n",
    "val_dataset = []\n",
    "for val_index in val_indices:\n",
    "    smiles = train_df['SMILES'].iloc[val_index]\n",
    "    labels = train_df['MLM'].iloc[val_index]\n",
    "    sample = {\n",
    "        'mol': Chem.MolFromSmiles(smiles),\n",
    "        'labels' : labels\n",
    "    }\n",
    "    val_dataset.append(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_collate_fn_config = DeMOLTaConfig(\n",
    "    do_masking=False,\n",
    "    smiles_only=False,\n",
    ")\n",
    "finetune_collate_fn = DeMOLTaCollateFn(finetune_collate_fn_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, collate_fn=finetune_collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE*2, collate_fn=finetune_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator_config = DeMOLTaConfig(\n",
    "    num_layers=12,\n",
    "    hidden_dim=384,\n",
    "    ff_dim=1536,\n",
    "    num_heads=6,\n",
    "    dropout=0.1,\n",
    "    progressive_layer_drop_prob=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_model = LitDeMOLTaDiscriminatorForSMILESClassification(\n",
    "    discriminator_config=discriminator_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lit_model.discriminator.load_state_dict(torch.load('./checkpoint/current_step=350000-Loss_G=0.0000-Loss_D=18.3434-discriminator.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='Val_Loss',\n",
    "    dirpath='./checkpoint/',\n",
    "    filename='DeMOLTa-discriminator-{epoch:02d}-{Loss:.2f}-{Val_Loss:.2f}',\n",
    "    save_top_k=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\dust\\anaconda3\\envs\\py310_torch2\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\logger_connector\\logger_connector.py:67: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "  warning_cache.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(\n",
    "    accelerator='gpu',\n",
    "    precision='16-mixed',\n",
    "    max_epochs=10,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    accumulate_grad_batches=16\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 3070') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "c:\\Users\\dust\\anaconda3\\envs\\py310_torch2\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:615: UserWarning: Checkpoint directory C:\\Users\\dust\\Documents\\Dacon\\med\\unimolplusplus\\checkpoint exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type                 | Params\n",
      "-------------------------------------------------------\n",
      "0 | discriminator | DeMOLTaDiscriminator | 53.4 M\n",
      "1 | classfier     | Linear               | 385   \n",
      "-------------------------------------------------------\n",
      "53.4 M    Trainable params\n",
      "0         Non-trainable params\n",
      "53.4 M    Total params\n",
      "213.504   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e869e32b0e54ce09dd42cacfa41ccb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dust\\anaconda3\\envs\\py310_torch2\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\dust\\anaconda3\\envs\\py310_torch2\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b2d0a040c494c39a20cb92f72a04d48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e940a1a8d1db428da99fabdd2ca23b56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a8a0f6b020b4e59a93c490a4661474b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfaa41dde79043d38b5bcbd2a993aaf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0ca70f69d3f48e0b1683534ea20fa65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(lit_model, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_config = DeMOLTaConfig(\n",
    "    num_layers=6,\n",
    "    hidden_dim=384,\n",
    "    ff_dim=1536,\n",
    "    num_heads=6,\n",
    "    dropout=0.0,\n",
    "    progressive_layer_drop_prob=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_model = LitDeMOLTaGeneratorForSMILESClassification(\n",
    "    generator_config=generator_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lit_model.generator.load_state_dict(torch.load('./checkpoint/current_step=350000-Loss_G=0.0000-Loss_D=18.3434-generator.pt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='Val_Loss',\n",
    "    dirpath='./checkpoint/',\n",
    "    filename='DeMOLTa-generator-{epoch:02d}-{Loss:.2f}-{Val_Loss:.2f}',\n",
    "    save_top_k=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dust\\anaconda3\\envs\\py310_torch2\\lib\\site-packages\\lightning\\fabric\\connector.py:554: UserWarning: 16 is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\dust\\anaconda3\\envs\\py310_torch2\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\logger_connector\\logger_connector.py:67: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "  warning_cache.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(\n",
    "    accelerator='gpu',\n",
    "    precision=16,\n",
    "    max_epochs=10,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    accumulate_grad_batches=16,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 3070') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "c:\\Users\\dust\\anaconda3\\envs\\py310_torch2\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:615: UserWarning: Checkpoint directory C:\\Users\\dust\\Documents\\Dacon\\med\\unimolplusplus\\checkpoint exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | generator | DeMOLTaGenerator | 26.9 M\n",
      "1 | classfier | Linear           | 385   \n",
      "-----------------------------------------------\n",
      "26.9 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.9 M    Total params\n",
      "107.594   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45f559a02cb94446bf033c83b75975c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dust\\anaconda3\\envs\\py310_torch2\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\dust\\anaconda3\\envs\\py310_torch2\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c297aa0b6d3c47f7bb4aa35205461529",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(lit_model, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_config = DeMOLTaConfig(\n",
    "    num_layers=6,\n",
    "    hidden_dim=384,\n",
    "    ff_dim=1536,\n",
    "    num_heads=6,\n",
    "    dropout=0.0,\n",
    "    progressive_layer_drop_prob=0.0\n",
    ")\n",
    "discriminator_config = DeMOLTaConfig(\n",
    "    num_layers=12,\n",
    "    hidden_dim=384,\n",
    "    ff_dim=1536,\n",
    "    num_heads=6,\n",
    "    dropout=0.0,\n",
    "    progressive_layer_drop_prob=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_model = LitDeMOLTaForSMILESClassification(\n",
    "    generator_config=generator_config,\n",
    "    discriminator_config=discriminator_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lit_model.generator.load_state_dict(torch.load('./checkpoint/current_step=350000-Loss_G=0.0000-Loss_D=18.3434-generator.pt'))\n",
    "lit_model.discriminator.load_state_dict(torch.load('./checkpoint/current_step=350000-Loss_G=0.0000-Loss_D=18.3434-discriminator.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='Val_Loss',\n",
    "    dirpath='./checkpoint/',\n",
    "    filename='DeMOLTa-scratch-{epoch:02d}-{Loss:.2f}-{Val_Loss:.2f}',\n",
    "    save_top_k=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dust\\anaconda3\\envs\\py310_torch2\\lib\\site-packages\\lightning\\fabric\\connector.py:554: UserWarning: 16 is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\dust\\anaconda3\\envs\\py310_torch2\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\logger_connector\\logger_connector.py:67: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "  warning_cache.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(\n",
    "    accelerator='gpu',\n",
    "    precision=16,\n",
    "    max_epochs=10,\n",
    "    callbacks=[checkpoint_callback],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 3070') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "c:\\Users\\dust\\anaconda3\\envs\\py310_torch2\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:615: UserWarning: Checkpoint directory C:\\Users\\dust\\Documents\\Dacon\\med\\unimolplusplus\\checkpoint exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type                 | Params\n",
      "-------------------------------------------------------\n",
      "0 | generator     | DeMOLTaGenerator     | 26.9 M\n",
      "1 | discriminator | DeMOLTaDiscriminator | 53.4 M\n",
      "2 | classfier     | Linear               | 385   \n",
      "-------------------------------------------------------\n",
      "80.3 M    Trainable params\n",
      "0         Non-trainable params\n",
      "80.3 M    Total params\n",
      "321.096   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dust\\anaconda3\\envs\\py310_torch2\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dust\\Documents\\Dacon\\med\\unimolplusplus\\trainer.py:179: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([4, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = F.mse_loss(x, labels) ** 0.5\n",
      "c:\\Users\\dust\\anaconda3\\envs\\py310_torch2\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 1/1399 [00:00<02:32,  9.15it/s, v_num=61, Loss=84.80]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dust\\Documents\\Dacon\\med\\unimolplusplus\\trainer.py:166: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = F.mse_loss(x, labels) ** 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  20%|█▉        | 277/1399 [00:28<01:55,  9.71it/s, v_num=61, Loss=48.20, Val_Loss=34.50] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  21%|██▏       | 300/1399 [00:31<01:54,  9.62it/s, v_num=61, Loss=32.00, Val_Loss=34.50]"
     ]
    }
   ],
   "source": [
    "trainer.fit(lit_model, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310_torch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
